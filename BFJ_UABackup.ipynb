{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnyQvUETCnyWOWJtJVHj0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliasoblomov/unify_test/blob/main/BFJ_UABackup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-analytics-data==0.18.4\n",
        "!pip install google-cloud-bigquery\n",
        "!pip install google-auth==2.27.0\n",
        "!pip install google-auth-oauthlib\n",
        "!pip install google-auth-httplib2"
      ],
      "metadata": {
        "id": "hF-HvogwEyge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e27NDOc_a-4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:bounceRate'},\n",
        "                        {'expression': 'ga:sessionDuration'},\n",
        "                        {'expression': 'ga:goalCompletionsAll'},\n",
        "                        {'expression': 'ga:pageviewsPerSession'},\n",
        "                        {'expression': 'ga:avgTimeOnPage'},\n",
        "                        {'expression': 'ga:totalEvents'},\n",
        "                        {'expression': 'ga:hits'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "            for i, metric in enumerate(metricHeaders):\n",
        "                row_data[metric['name']] = float(row['metrics'][0]['values'][i]) if 'values' in row['metrics'][0] else 0.0\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Rename columns to match BigQuery schema\n",
        "    df = df.rename(columns={\n",
        "        'ga:date': 'date',\n",
        "        'ga:bounceRate': 'bounceRate',\n",
        "        'ga:sessionDuration': 'sessionDuration',\n",
        "        'ga:goalCompletionsAll': 'goalCompletionsAll',\n",
        "        'ga:pageviewsPerSession': 'pageviewsPerSession',\n",
        "        'ga:avgTimeOnPage': 'avgTimeOnPage',\n",
        "        'ga:totalEvents': 'totalEvents',\n",
        "        'ga:hits': 'hits'\n",
        "    })\n",
        "\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"bounceRate\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"sessionDuration\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"goalCompletionsAll\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"pageviewsPerSession\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"avgTimeOnPage\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"totalEvents\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"hits\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['bounceRate'] = df['bounceRate'].astype(float)\n",
        "    df['sessionDuration'] = df['sessionDuration'].astype(float)\n",
        "    df['goalCompletionsAll'] = df['goalCompletionsAll'].astype(float)\n",
        "    df['pageviewsPerSession'] = df['pageviewsPerSession'].astype(float)\n",
        "    df['avgTimeOnPage'] = df['avgTimeOnPage'].astype(float)\n",
        "    df['totalEvents'] = df['totalEvents'].astype(int)\n",
        "    df['hits'] = df['hits'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_metrics_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:bounceRate'},\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:pageviewsPerSession'}\n",
        "                    ],\n",
        "                    'dimensions': [\n",
        "                        {'name': 'ga:date'},\n",
        "                        {'name': 'ga:userAgeBracket'},\n",
        "                        {'name': 'ga:userGender'}\n",
        "                    ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "            for i, metric in enumerate(metricHeaders):\n",
        "                row_data[metric['name']] = float(row['metrics'][0]['values'][i]) if 'values' in row['metrics'][0] else 0.0\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Rename columns to match BigQuery schema\n",
        "    df = df.rename(columns={\n",
        "        'ga:date': 'date',\n",
        "        'ga:userAgeBracket': 'userAgeBracket',\n",
        "        'ga:userGender': 'userGender',\n",
        "        'ga:users': 'users',\n",
        "        'ga:sessions': 'sessions',\n",
        "        'ga:bounceRate': 'bounceRate',\n",
        "        'ga:pageviews': 'pageviews',\n",
        "        'ga:pageviewsPerSession': 'pageviewsPerSession'\n",
        "    })\n",
        "\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"userAgeBracket\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"userGender\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"bounceRate\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"pageviews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"pageviewsPerSession\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['users'] = df['users'].astype(int)\n",
        "    df['sessions'] = df['sessions'].astype(int)\n",
        "    df['bounceRate'] = df['bounceRate'].astype(float)\n",
        "    df['pageviews'] = df['pageviews'].astype(int)\n",
        "    df['pageviewsPerSession'] = df['pageviewsPerSession'].astype(float)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_user_metrics_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "XvjNhSmKBmeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:channelGrouping'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'channelGrouping': dim1,\n",
        "                'sessions': int(row['metrics'][0]['values'][0]),\n",
        "                'users': int(row['metrics'][0]['values'][1]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][2]),\n",
        "                'uniquePurchases': int(row['metrics'][0]['values'][3]),\n",
        "                'transactionRevenue': float(row['metrics'][0]['values'][4])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"channelGrouping\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"uniquePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"transactionRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['sessions'] = df['sessions'].astype(int)\n",
        "    df['users'] = df['users'].astype(int)\n",
        "    df['newUsers'] = df['newUsers'].astype(int)\n",
        "    df['uniquePurchases'] = df['uniquePurchases'].astype(int)\n",
        "    df['transactionRevenue'] = df['transactionRevenue'].astype(float)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_channel_group_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "nFOWjMTDChFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:country'},\n",
        "                                   {'name': 'ga:language'}\n",
        "                                   ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            dim2 = row['dimensions'][2]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'country': dim1,\n",
        "                'language': dim2,\n",
        "                'sessions': int(row['metrics'][0]['values'][0]),\n",
        "                'pageviews': int(row['metrics'][0]['values'][1]),\n",
        "                'users': int(row['metrics'][0]['values'][2]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][3])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"country\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"language\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"pageviews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Upload the UA DataFrame to BigQuery\n",
        "        upload_to_bigquery(ua_df, 'ua_data_geo_network')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qIdbkCiAClYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:totalEvents'},\n",
        "                        {'expression': 'ga:uniqueEvents'},\n",
        "                        {'expression': 'ga:eventValue'},\n",
        "                        {'expression': 'ga:sessionsWithEvent'}\n",
        "                    ],\n",
        "                    'dimensions': [\n",
        "                        {'name': 'ga:date'},\n",
        "                        {'name': 'ga:eventCategory'},\n",
        "                        {'name': 'ga:eventAction'},\n",
        "                        {'name': 'ga:eventLabel'}\n",
        "                    ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "            for i, metric in enumerate(metricHeaders):\n",
        "                row_data[metric['name']] = int(row['metrics'][0]['values'][i]) if 'values' in row['metrics'][0] else 0\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Rename columns to match BigQuery schema\n",
        "    df = df.rename(columns={\n",
        "        'ga:date': 'date',\n",
        "        'ga:eventCategory': 'eventCategory',\n",
        "        'ga:eventAction': 'eventAction',\n",
        "        'ga:eventLabel': 'eventLabel',\n",
        "        'ga:totalEvents': 'totalEvents',\n",
        "        'ga:uniqueEvents': 'uniqueEvents',\n",
        "        'ga:eventValue': 'eventValue',\n",
        "        'ga:sessionsWithEvent': 'sessionsWithEvent'\n",
        "    })\n",
        "\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"eventCategory\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"eventAction\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"eventLabel\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"totalEvents\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"uniqueEvents\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"eventValue\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"sessionsWithEvent\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['totalEvents'] = df['totalEvents'].astype(int)\n",
        "    df['uniqueEvents'] = df['uniqueEvents'].astype(int)\n",
        "    df['eventValue'] = df['eventValue'].astype(int)\n",
        "    df['sessionsWithEvent'] = df['sessionsWithEvent'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_event_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "JNN1R5OaCviC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:goal1Completions'},\n",
        "                        {'expression': 'ga:goal2Completions'},\n",
        "                        {'expression': 'ga:goal3Completions'},\n",
        "                        {'expression': 'ga:goal4Completions'},\n",
        "                        {'expression': 'ga:goal5Completions'},\n",
        "                    ],\n",
        "                    'dimensions': [\n",
        "                        {'name': 'ga:date'},\n",
        "                        {'name': 'ga:source'},\n",
        "                        {'name': 'ga:campaign'},\n",
        "                        {'name': 'ga:medium'}\n",
        "                    ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            dim2 = row['dimensions'][2]\n",
        "            dim3 = row['dimensions'][3]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'source': dim1,\n",
        "                'campaign': dim2,\n",
        "                'medium': dim3,\n",
        "                'goal1Completions': int(row['metrics'][0]['values'][0]),\n",
        "                'goal2Completions': int(row['metrics'][0]['values'][1]),\n",
        "                'goal3Completions': int(row['metrics'][0]['values'][2]),\n",
        "                'goal4Completions': int(row['metrics'][0]['values'][3]),\n",
        "                'goal5Completions': int(row['metrics'][0]['values'][4])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"source\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"campaign\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"medium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"goal1Completions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"goal2Completions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"goal3Completions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"goal4Completions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"goal5Completions\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_goal_completions')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0vF-3tXgCzqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:itemQuantity'},\n",
        "                        {'expression': 'ga:itemRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:productName'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'productName': dim1,\n",
        "                'itemQuantity': int(row['metrics'][0]['values'][0]),\n",
        "                'itemRevenue': float(row['metrics'][0]['values'][1])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"productName\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"itemQuantity\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"itemRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_item_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "PsMhko0EC3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:landingPagePath'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'landingPage': dim1,\n",
        "                'sessions': int(row['metrics'][0]['values'][0]),\n",
        "                'totalUsers': int(row['metrics'][0]['values'][1]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][2]),\n",
        "                'ecommercePurchases': int(row['metrics'][0]['values'][3]),\n",
        "                'purchaseRevenue': float(row['metrics'][0]['values'][4])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"landingPage\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"ecommercePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"purchaseRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_landingpage_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "WbmaF57AC9FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:pagePath'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'pagePath': dim1,\n",
        "                'pageviews': int(row['metrics'][0]['values'][0]),\n",
        "                'users': int(row['metrics'][0]['values'][1])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"pagePath\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"pageviews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_pagepath_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "LO6yAPvGDALL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [{'expression': 'ga:pageviews'}],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            row_data = {header: row['dimensions'][i] for i, header in enumerate(dimensionHeaders)}\n",
        "            for i, metric in enumerate(metricHeaders):\n",
        "                row_data[metric['name']] = int(row['metrics'][0]['values'][i]) if 'values' in row['metrics'][0] else 0\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Rename columns to match BigQuery schema\n",
        "    df = df.rename(columns={\n",
        "        'ga:date': 'date',\n",
        "        'ga:pageviews': 'pageviews'\n",
        "    })\n",
        "\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"pageviews\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['pageviews'] = df['pageviews'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_pageviews_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "M9uXCt2lDCvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:pageviews'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'},\n",
        "                                   {'name': 'ga:browser'},\n",
        "                                   {'name': 'ga:operatingSystem'},\n",
        "                                   {'name': 'ga:deviceCategory'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            dim1 = row['dimensions'][1]\n",
        "            dim2 = row['dimensions'][2]\n",
        "            dim3 = row['dimensions'][3]\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'browser': dim1,\n",
        "                'operatingSystem': dim2,\n",
        "                'deviceCategory': dim3,\n",
        "                'sessions': int(row['metrics'][0]['values'][0]),\n",
        "                'pageviews': int(row['metrics'][0]['values'][1]),\n",
        "                'users': int(row['metrics'][0]['values'][2]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][3])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"browser\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"operatingSystem\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"deviceCategory\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"pageviews\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['sessions'] = df['sessions'].astype(int)\n",
        "    df['pageviews'] = df['pageviews'].astype(int)\n",
        "    df['users'] = df['users'].astype(int)\n",
        "    df['newUsers'] = df['newUsers'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_platform_device_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "SH1k3Fr7DFFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'uniquePurchases': int(row['metrics'][0]['values'][0]),\n",
        "                'transactionRevenue': float(row['metrics'][0]['values'][1])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"uniquePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"transactionRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['uniquePurchases'] = df['uniquePurchases'].astype(int)\n",
        "    df['transactionRevenue'] = df['transactionRevenue'].astype(float)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_purchases_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Y38J5zuODHQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [{'expression': 'ga:sessions'}],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'sessions': int(row['metrics'][0]['values'][0])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['sessions'] = df['sessions'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_sessions_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "xLF5umT7DKcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:sessions'},\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'},\n",
        "                        {'expression': 'ga:uniquePurchases'},\n",
        "                        {'expression': 'ga:transactionRevenue'}\n",
        "                    ],\n",
        "                    'dimensions': [\n",
        "                        {'name': 'ga:date'},\n",
        "                        {'name': 'ga:source'},\n",
        "                        {'name': 'ga:campaign'},\n",
        "                        {'name': 'ga:medium'}\n",
        "                    ],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'source': row['dimensions'][1],\n",
        "                'campaign': row['dimensions'][2],\n",
        "                'medium': row['dimensions'][3],\n",
        "                'sessions': int(row['metrics'][0]['values'][0]),\n",
        "                'users': int(row['metrics'][0]['values'][1]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][2]),\n",
        "                'uniquePurchases': int(row['metrics'][0]['values'][3]),\n",
        "                'transactionRevenue': float(row['metrics'][0]['values'][4])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"source\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"campaign\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"medium\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"sessions\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"users\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"uniquePurchases\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"transactionRevenue\", \"FLOAT\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['sessions'] = df['sessions'].astype(int)\n",
        "    df['users'] = df['users'].astype(int)\n",
        "    df['newUsers'] = df['newUsers'].astype(int)\n",
        "    df['uniquePurchases'] = df['uniquePurchases'].astype(int)\n",
        "    df['transactionRevenue'] = df['transactionRevenue'].astype(float)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_source_campaign_medium_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "GNJwzgzDDPPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Load configuration from a JSON file\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# UA Variables\n",
        "SCOPES_UA = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = config['SERVICE_ACCOUNT_FILE']\n",
        "VIEW_ID = config['VIEW_ID_UA']\n",
        "BIGQUERY_PROJECT = config['BIGQUERY_PROJECT']\n",
        "BIGQUERY_DATASET = config['DATASET_ID']\n",
        "\n",
        "# Setting up the environment variable for Google Application Credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = KEY_FILE_LOCATION\n",
        "\n",
        "def initialize_analyticsreporting():\n",
        "    \"\"\"Initializes the Google Analytics Reporting API client.\"\"\"\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_name(KEY_FILE_LOCATION, SCOPES_UA)\n",
        "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "    return analytics\n",
        "\n",
        "def get_ua_report(analytics):\n",
        "    \"\"\"Fetches the report data from Google Analytics UA.\"\"\"\n",
        "    return analytics.reports().batchGet(\n",
        "        body={\n",
        "            'reportRequests': [\n",
        "                {\n",
        "                    'viewId': VIEW_ID,\n",
        "                    'dateRanges': [{'startDate': config['UA_INITIAL_FETCH_FROM_DATE'], 'endDate': config['UA_FETCH_TO_DATE']}],\n",
        "                    'metrics': [\n",
        "                        {'expression': 'ga:users'},\n",
        "                        {'expression': 'ga:newUsers'}\n",
        "                    ],\n",
        "                    'dimensions': [{'name': 'ga:date'}],\n",
        "                    'pageSize': 10000000\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "def response_to_dataframe(response):\n",
        "    list_rows = []\n",
        "\n",
        "    for report in response.get('reports', []):\n",
        "        columnHeader = report.get('columnHeader', {})\n",
        "        dimensionHeaders = columnHeader.get('dimensions', [])\n",
        "        metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
        "        for row in report.get('data', {}).get('rows', []):\n",
        "            try:\n",
        "                date_value = pd.to_datetime(row['dimensions'][0], format='%Y%m%d')\n",
        "            except ValueError:\n",
        "                date_value = pd.NaT\n",
        "            row_data = {\n",
        "                'date': date_value,\n",
        "                'totalUsers': int(row['metrics'][0]['values'][0]),\n",
        "                'newUsers': int(row['metrics'][0]['values'][1])\n",
        "            }\n",
        "            list_rows.append(row_data)\n",
        "    return pd.DataFrame(list_rows)\n",
        "\n",
        "def upload_to_bigquery(df, table_name):\n",
        "    # Create schema for the DataFrame\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"totalUsers\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"newUsers\", \"INTEGER\")\n",
        "    ]\n",
        "\n",
        "    # Check that all schema fields are in the DataFrame\n",
        "    df_columns = set(df.columns)\n",
        "    schema_fields = set(field.name for field in schema)\n",
        "    if not schema_fields.issubset(df_columns):\n",
        "        raise ValueError(f\"Schema fields not present in DataFrame: {schema_fields - df_columns}\")\n",
        "\n",
        "    # Convert the 'date' column from string to datetime\n",
        "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "\n",
        "    # Ensure data types match expected BigQuery types\n",
        "    df['totalUsers'] = df['totalUsers'].astype(int)\n",
        "    df['newUsers'] = df['newUsers'].astype(int)\n",
        "\n",
        "    # Print data types for verification\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Configure BigQuery job to partition the table by the 'date' column\n",
        "    table_ref = f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field='date'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Upload to BigQuery\n",
        "    bq_client = bigquery.Client()\n",
        "    bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config).result()\n",
        "    print(f\"Data uploaded and partitioned by date to {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # UA processing\n",
        "        analytics = initialize_analyticsreporting()\n",
        "        ua_response = get_ua_report(analytics)\n",
        "        ua_df = response_to_dataframe(ua_response)\n",
        "\n",
        "        # Check DataFrame content\n",
        "        if ua_df.empty:\n",
        "            print(\"No data found in the UA response.\")\n",
        "        else:\n",
        "            print(f\"DataFrame columns: {ua_df.columns}\")\n",
        "            print(f\"Sample data:\\n{ua_df.head()}\")\n",
        "\n",
        "            # Upload the UA DataFrame to BigQuery\n",
        "            upload_to_bigquery(ua_df, 'ua_users_newusers_data')\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "AtHsv-_5DVWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}